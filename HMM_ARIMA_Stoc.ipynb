{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yy0750/Stock-Price-Prediction-using-HMM/blob/main/%08%08HMM_ARIMA_Stoc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix A.1. Preparation for Model Construction/Modeling"
      ],
      "metadata": {
        "id": "PFDN8tF9RhgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appendix A.1.1. Importing Necessary Modeules"
      ],
      "metadata": {
        "id": "q4R6l-WAR0OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hmmlearn\n",
        "!pip install pmdarima"
      ],
      "metadata": {
        "id": "kyxSSwJ36Mad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "import yfinance as yf\n",
        "import pmdarima as pm\n",
        "from hmmlearn import hmm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from pmdarima.arima import ndiffs\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA"
      ],
      "metadata": {
        "id": "Gsb6XCp9EaZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = '2021-12-10'\n",
        "end_date = '2023-11-17'\n",
        "\n",
        "stock_code = '005930.KS' #Sk하이닉스: '00660.KS'\n",
        "df = yf.download(stock_code, start=start_date, end=end_date)\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "df.insert(0, 'ID', df.index)\n",
        "df.drop('Adj Close', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "jpyUBQBEo2hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Appendix A.1.3 Data formatting and Normalization"
      ],
      "metadata": {
        "id": "LGsLgzUQR7Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Avg'] = pd.Series([((row.Open + row.High + row.Low + row.Close)/4) for index, row in df.iterrows()])\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
        "\n",
        "X = df.values[:, [0, 7]]\n",
        "X = np.nan_to_num(X)\n",
        "X = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "pe-a6UlQSbIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appendix A.1.4 Clustering\n",
        "###### K-Means Clustering\n",
        "###### Preliminary Clustering"
      ],
      "metadata": {
        "id": "qgOcLTpSSbek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78I0gZlu5xIG"
      },
      "outputs": [],
      "source": [
        "clusterNum = 3\n",
        "k_means = KMeans(init = 'k-means++', n_clusters = clusterNum, n_init = 12)\n",
        "k_means.fit(X)\n",
        "labels = k_means.labels_\n",
        "\n",
        "df['State'] = labels\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels.astype(np.float64), alpha=0.5);\n",
        "plt.xlabel('S.no');\n",
        "plt.ylabel('Price');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pV2BLdSzE7oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array(df['Avg'].dropna()).reshape(-1, 1)\n",
        "\n",
        "subclusters = {}\n",
        "\n",
        "for cluster_id in np.unique(labels):\n",
        "\n",
        "    cluster_data = data[labels == cluster_id]\n",
        "    cluster_mean = np.mean(cluster_data, axis=0)\n",
        "    cluster_covariance = np.cov(cluster_data, rowvar=False)\n",
        "    subclusters[cluster_id] = {\n",
        "        'means': cluster_mean,\n",
        "        'covariances': cluster_covariance\n",
        "    }\n",
        "\n",
        "pre_means = np.empty(3)\n",
        "pre_means[0] = subclusters[0]['means']\n",
        "pre_means[1] = subclusters[1]['means']\n",
        "pre_means[2] = subclusters[2]['means']\n",
        "\n",
        "pre_covars = np.empty(3)\n",
        "pre_covars[0] = subclusters[0]['covariances']\n",
        "pre_covars[1] = subclusters[1]['covariances']\n",
        "pre_covars[2] = subclusters[2]['covariances']"
      ],
      "metadata": {
        "id": "GRsX2K_ra7f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Clustering the log-likelihoods\n"
      ],
      "metadata": {
        "id": "Q6hgQCnSTceN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_prices = df['Avg'].tolist()\n",
        "\n",
        "pre_training_ll = [np.nan] * len(pre_prices)\n",
        "\n",
        "for i in range(3, len(pre_prices)):  #for i in range((len(pre_prices))): - Method 2\n",
        "    print('Datapoint: ', i)\n",
        "    #Initialize\n",
        "    pre_model = hmm.GMMHMM(n_components = 3, n_mix=1, n_iter = 100, covariance_type='diag', init_params='')\n",
        "    pre_model.startprob_ = np.array([1/3, 1/3, 1/3])\n",
        "    pre_model.transmat_ = np.array([[1/3, 1/3, 1/3], [1/3, 1/3, 1/3], [1/3, 1/3, 1/3]])\n",
        "    pre_model.means_ = np.array([pre_means[0], pre_means[1], pre_means[2]]).reshape(3, 1)\n",
        "    pre_model.covars_ = np.array([pre_covars[0], pre_covars[1], pre_covars[2]]).reshape(3, 1)\n",
        "    pre_model.weights_ = np.array([1, 1, 1]).reshape(3,1)\n",
        "\n",
        "    # Samples\n",
        "    X = np.array(pre_prices[max(i-4, 0):i]).reshape(i - max(i-4, 0),1)\n",
        "\n",
        "    # Method 2\n",
        "    #X = np.array([[df['Open'][i]], df['Close'][i]], df['High'][i]], df['Low'][i]]).reshape(4, 1)\n",
        "\n",
        "    X = np.nan_to_num(X)\n",
        "\n",
        "    # Training\n",
        "    try:\n",
        "        pre_model.fit(X)\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "    pre_training_ll[i] = pre_model.monitor_.history[0]\n",
        "\n",
        "df['pre_ll'] = np.nan_to_num(pre_training_ll)"
      ],
      "metadata": {
        "id": "Yv9LcLF9Tnj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Refining the clusters (Clustering the log_likelihoods):"
      ],
      "metadata": {
        "id": "nTE_eTEbTiWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.values[:, [0, 9]]\n",
        "X = np.nan_to_num(X)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "clusterNum = 3\n",
        "k_means = KMeans(init = 'k-means++', n_clusters = clusterNum, n_init = 12)\n",
        "k_means.fit(X)\n",
        "labels = k_means.labels_\n",
        "\n",
        "df['State'] = labels\n",
        "\n",
        "plt.scatter(df['ID'], df['Avg'], c=df['State'].astype(np.float64), alpha=0.5);\n",
        "plt.xlabel('S.no');\n",
        "plt.ylabel('Price');"
      ],
      "metadata": {
        "id": "ODzoJ99x526t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix A.2. Model Construction/Modeling\n"
      ],
      "metadata": {
        "id": "1mWPACy2UFSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appendix A.2.1 Mixture Model\n",
        "##### Low Economy"
      ],
      "metadata": {
        "id": "obkcVgkQUSFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_df = df[df.State == 0]\n",
        "\n",
        "low_df.reset_index(inplace=True)\n",
        "low_df.drop(['ID'], axis=1, inplace=True)\n",
        "low_df['ID'] = low_df.index\n",
        "\n",
        "X = low_df.values[:, [10, 7]]\n",
        "X = np.nan_to_num(X)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "clusterNum = 2\n",
        "low_k_means = KMeans(init='k-means++', n_clusters=clusterNum, n_init=12)\n",
        "low_k_means.fit(X)\n",
        "labels = low_k_means.labels_\n",
        "low_df['Mix'] = labels\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels.astype(np.float64), alpha=0.5);\n",
        "plt.xlabel('S.no');\n",
        "plt.ylabel('Price');\n",
        "\n",
        "low_means = []\n",
        "low_covars = []\n",
        "low_weights = []\n",
        "\n",
        "for i in [0, 1]:\n",
        "    tdf = low_df[low_df.Mix == i]\n",
        "\n",
        "    #Mean\n",
        "    low_means.append(tdf['Avg'].sum() / tdf['Avg'].count())\n",
        "\n",
        "    #Variance\n",
        "    low_covars.append(tdf['Avg'].std() ** 2)\n",
        "\n",
        "    #Weights\n",
        "    tdf = low_df[low_df.Mix == 0]\n",
        "    low_weights.append(tdf['Avg'].count() / low_df['Avg'].count())\n",
        "    low_weights.append(1 - low_weights[0])"
      ],
      "metadata": {
        "id": "KGB5nUD29Ea1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Medium Economy"
      ],
      "metadata": {
        "id": "_RNzbUapUcID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medium_df = df[df.State == 2]\n",
        "\n",
        "medium_df.reset_index(inplace=True)\n",
        "medium_df.drop(['ID'], axis=1, inplace=True)\n",
        "medium_df['ID'] = medium_df.index\n",
        "\n",
        "X = medium_df.values[:, [10, 7]]\n",
        "X = np.nan_to_num(X)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "clusterNum = 2\n",
        "medium_k_means = KMeans(init='k-means++', n_clusters=clusterNum, n_init=12)\n",
        "medium_k_means.fit(X)\n",
        "labels = medium_k_means.labels_\n",
        "medium_df['Mix'] = labels\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels.astype(np.float64), alpha=0.5)\n",
        "plt.xlabel('S.no')\n",
        "plt.ylabel('Price')\n",
        "\n",
        "medium_means = []\n",
        "medium_covars = []\n",
        "medium_weights = []\n",
        "\n",
        "for i in [0, 1]:\n",
        "    tdf = medium_df[medium_df.Mix == i]\n",
        "\n",
        "    #Mean\n",
        "    medium_means.append(tdf['Avg'].sum() / tdf['Avg'].count())\n",
        "\n",
        "    #Variance\n",
        "    medium_covars.append(tdf['Avg'].std() ** 2)\n",
        "\n",
        "    #Weights\n",
        "    tdf = medium_df[medium_df.Mix == 0]\n",
        "    medium_weights.append(tdf['Avg'].count() / medium_df['Avg'].count())\n",
        "    medium_weights.append(1 - medium_weights[0])"
      ],
      "metadata": {
        "id": "oBLvbCfS992v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### High Economy"
      ],
      "metadata": {
        "id": "yrx1hSHyUfmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_df = df[df.State == 1]\n",
        "\n",
        "high_df.reset_index(inplace=True)\n",
        "high_df.drop(['ID'], axis=1, inplace=True)\n",
        "high_df['ID'] = high_df.index\n",
        "\n",
        "X = high_df.values[:, [10, 7]]\n",
        "X = np.nan_to_num(X)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "clusterNum = 2\n",
        "high_K_means = KMeans(init = 'k-means++', n_clusters = clusterNum, n_init = 12)\n",
        "high_K_means.fit(X)\n",
        "labels = high_K_means.labels_\n",
        "high_df['Mix'] = labels\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels.astype(np.float64), alpha=0.5)\n",
        "plt.xlabel('S.no')\n",
        "plt.ylabel('Price')\n",
        "\n",
        "high_means = []\n",
        "high_covars = []\n",
        "high_weights = []\n",
        "\n",
        "for i in [0, 1]:\n",
        "    tdf = high_df[high_df.Mix == i]\n",
        "\n",
        "    #Mean\n",
        "    high_means.append(tdf['Avg'].sum() / tdf['Avg'].count())\n",
        "\n",
        "    #Variance\n",
        "    high_covars.append(tdf['Avg'].std() ** 2)\n",
        "\n",
        "    #Weights\n",
        "    tdf = high_df[high_df.Mix == 0]\n",
        "    high_weights.append(tdf['Avg'].count() / high_df['Avg'].count())\n",
        "    high_weights.append(1 - high_weights[0])"
      ],
      "metadata": {
        "id": "h-pu_C9A-xp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appendix A.2.2 HMM Training and Testing"
      ],
      "metadata": {
        "id": "5wd58EH2UkK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prices = df['Close'].tolist()\n",
        "\n",
        "training_ll = [None] * len(prices)\n",
        "lls = [None] * len(prices)\n",
        "predicted_prices = [None] * (len(prices)+1)\n",
        "epsilon = 0.2\n",
        "curr_ll = 0\n",
        "\n",
        "for i in range(3, len(prices)): #{for i in range(len(prices)):} - Method 2\n",
        "    print('Datapoint: ', i)\n",
        "    #Initialize\n",
        "    model = hmm.GMMHMM(n_components=3, n_mix=2, n_iter=100, covariance_type='diag', init_params='')\n",
        "    model.startprob_ = np.array([1/3, 1/3, 1/3])\n",
        "    model.transmat_ = np.array([[1/3, 1/3, 1/3], [1/3, 1/3, 1/3], [1/3, 1/3, 1/3]])\n",
        "\n",
        "    model.means_ = np.array([[medium_means[0], medium_means[1]], [high_means[0], high_means[1]], [low_means[0], low_means[1]]]).reshape(3, 2, 1)\n",
        "    model.covars_ = np.array([[medium_covars[0], medium_covars[1]], [high_covars[0], high_covars[1]], [low_covars[0], low_covars[1]]]).reshape(3, 2, 1)\n",
        "    model.weights_ = np.array([[medium_weights[0], medium_weights[1]], [high_weights[0], high_weights[1]], [low_weights[0], low_weights[1]]])\n",
        "\n",
        "    #Samples\n",
        "    X = np.array(prices[max(i-4, 0):i]).reshape(i - max(i-4, 0),1)\n",
        "\n",
        "    #Method 2\n",
        "    #X = np.array([df['Open'][i], df['Close'][i], df['High'][i], df['Low'][i]]).reshape(4, 1)\n",
        "    X = np.nan_to_num(X)\n",
        "\n",
        "    #Training\n",
        "    try:\n",
        "        model.fit(X)\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "\n",
        "    ll_arr = np.array(model.monitor_.history)\n",
        "    ll_arr = ll_arr[~np.isnan(ll_arr)]\n",
        "\n",
        "    # Log-likelihood\n",
        "    try:\n",
        "        #curr_ll = ll_arr[0] # - First Iteration\n",
        "        #curr_ll = np.median(ll_arr) # - Middle Iteration\n",
        "        curr_ll = ll_arr[-1] # - Last Iteration\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    lls[i] = (curr_ll)\n",
        "\n",
        "    #Predict\n",
        "    if i > 427:\n",
        "        diff = []\n",
        "        for j in range(3, i): #{for j in range(i):} - Method 2\n",
        "            if lls[j] != None:\n",
        "                if lls[j] < (curr_ll + epsilon) and lls[j] > (curr_ll - epsilon):\n",
        "                    diff.append(prices[j+1] - prices[j])\n",
        "\n",
        "        if len(diff) > 0:\n",
        "            min_diff = diff[np.argmin(np.abs(diff))]\n",
        "            predicted_prices[i+1] = prices[i] + min_diff\n",
        "\n",
        "df['PredictedPrices'] = pd.Series(predicted_prices)"
      ],
      "metadata": {
        "id": "vWuF3EgM-SvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appendix A.2.3 Model Performance"
      ],
      "metadata": {
        "id": "B_vR--TuUtN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot\n",
        "plt.plot(df[df['Date'] > '31-08-2023']['Close'], label='Actual Prices', c='b')\n",
        "plt.plot(df['PredictedPrices'], label='GMM-HMM Prices', c='r')\n",
        "plt.title('Stock Price Prediction with GMM-HMM')\n",
        "plt.ylabel('Prices')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#Metrics\n",
        "y_true = df[~np.isnan(df['PredictedPrices'])]['Close'].tolist()\n",
        "y_pred = df[~np.isnan(df['PredictedPrices'])]['PredictedPrices'].tolist()\n",
        "\n",
        "print('MSE: ', metrics.mean_squared_error(y_true, y_pred))\n",
        "print('MAE: ', metrics.mean_absolute_error(y_true, y_pred))\n",
        "print('MAE%: ', metrics.mean_absolute_percentage_error(y_true, y_pred))\n",
        "print('R2 Score: ', metrics.r2_score(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "4_VYSrhS_U7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA"
      ],
      "metadata": {
        "id": "zG5Vmy_fU1em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Data"
      ],
      "metadata": {
        "id": "oOuSlK56V3bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = '2021-12-10'\n",
        "end_date = '2023-11-17'\n",
        "\n",
        "stock_code = '005930.KS' #Sk하이닉스: '00660.KS'\n",
        "df = yf.download(stock_code, start=start_date, end=end_date)\n",
        "\n",
        "data = df['Adj Close'][df['Volume'] != 0]"
      ],
      "metadata": {
        "id": "IR1OfhW5AET5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 plot rolling"
      ],
      "metadata": {
        "id": "3elAvwG1V_kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_rolling(data, interval):\n",
        "    rolmean = data.rolling(interval).mean()\n",
        "    rolstd = data.rolling(interval).std()\n",
        "    #Plot rolling statistics:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.xlabel('Date')\n",
        "    orig = plt.plot(data, color='blue',label='Original')\n",
        "    mean = plt.plot(rolmean, color='red', label='Rolling Mean {}'.format(interval))\n",
        "    std = plt.plot(rolstd, color='black', label = 'Rolling Std {}'.format(interval))\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show()\n",
        "\n",
        "plot_rolling(data, 30)"
      ],
      "metadata": {
        "id": "IGBf6xEZVjj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 ADF Test"
      ],
      "metadata": {
        "id": "xnEM0l1dXAkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adf_test(data):\n",
        "    result = adfuller(data.values)\n",
        "    print('ADF Statistics: %f' % result[0])\n",
        "    print('p-value: %f' % result[1])\n",
        "    print('num of lags: %f' % result[2])\n",
        "    print('num of observations: %f' % result[3])\n",
        "    print('Critical values:')\n",
        "    for k, v in result[4].items():\n",
        "        print('\\t%s: %.3f' % (k,v))\n",
        "\n",
        "print('ADF TEST RESULT')\n",
        "adf_test(data)"
      ],
      "metadata": {
        "id": "zl_1RyW_WDL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2Intergrated"
      ],
      "metadata": {
        "id": "U8ep_sT4XXtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dff1 = data.diff().dropna()\n",
        "dff1.plot(figsize=(15,5))"
      ],
      "metadata": {
        "id": "zvGhpaEbXISG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adf_test(dff1)"
      ],
      "metadata": {
        "id": "FtNCivYAXjg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 ARIMA(p,d,q) parameter"
      ],
      "metadata": {
        "id": "7E_lsnpGXpE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = df['Close']\n",
        "n_diffs = ndiffs(data, alpha=0.05, test='adf', max_d=6)\n",
        "print(f\"d = {n_diffs}\")"
      ],
      "metadata": {
        "id": "lMA54ePEXlhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pm.auto_arima(\n",
        "            y=data,\n",
        "            d=1,\n",
        "            start_p=0, max_p=3,\n",
        "            start_q=0, max_q=3,\n",
        "            m=1, seasonal=False,\n",
        "            stepwise=True,\n",
        "            trace=True\n",
        ")"
      ],
      "metadata": {
        "id": "rU9nbmZNYJjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Fit Train Data"
      ],
      "metadata": {
        "id": "yIbH0QPFcBy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = data[:int(len(data)*0.9)], data[int(len(data)*0.9):]\n",
        "\n",
        "model_fit = pm.auto_arima(\n",
        "    \t    y=train_data,\n",
        " \t        d=n_diffs ,\n",
        "            start_p=0, max_p=2,\n",
        "            start_q=0, max_q=2,\n",
        "            m=1, seasonal=False,\n",
        "            stepwise=True,\n",
        "            trace=True\n",
        ")\n",
        "print(model_fit.summary())"
      ],
      "metadata": {
        "id": "Pm8kvVuuZ8wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Forecast Function"
      ],
      "metadata": {
        "id": "LGJjMp37fC2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forecast_n_step(model, n = 1):\n",
        "    fc, conf_int = model.predict(n_periods=n, return_conf_int=True)\n",
        "    # print(\"fc\", fc,\"conf_int\", conf_int)\n",
        "    return (\n",
        "        fc.tolist()[0:n], np.asarray(conf_int).tolist()[0:n]\n",
        "   )\n",
        "\n",
        "def forecast(len, model, index, data=None):\n",
        "    y_pred = []\n",
        "    pred_upper = []\n",
        "    pred_lower = []\n",
        "\n",
        "    if data is not None:\n",
        "        for new_ob in data:\n",
        "            fc, conf = forecast_n_step(model)\n",
        "            y_pred.append(fc[0])\n",
        "            pred_upper.append(conf[0][1])\n",
        "            pred_lower.append(conf[0][0])\n",
        "            model.update(new_ob)\n",
        "    else:\n",
        "        for i in range(len):\n",
        "            fc, conf = forecast_n_step(model)\n",
        "            y_pred.append(fc[0])\n",
        "            pred_upper.append(conf[0][1])\n",
        "            pred_lower.append(conf[0][0])\n",
        "            model.update(fc[0])\n",
        "    return pd.Series(y_pred, index=index), pred_upper, pred_lower"
      ],
      "metadata": {
        "id": "DG9-ejDwagIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 Forecast"
      ],
      "metadata": {
        "id": "P4jgFGgtfVgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fc, upper, lower = forecast(len(test_data), model_fit, test_data.index, data = test_data)\n",
        "\n",
        "plt.plot(test_data, label='Actual Price)')\n",
        "plt.plot(fc, color='red',label='ARIMA Predictions')\n",
        "plt.ylabel('Price')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aMNE0uhNcWDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 MAPE"
      ],
      "metadata": {
        "id": "mQzcWe27fcD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_test_data = np.log(test_data)\n",
        "log_fc = np.log(fc)\n",
        "\n",
        "mape = np.mean(np.abs(np.exp(log_fc) - np.exp(log_test_data)) / np.abs(np.exp(log_test_data)))\n",
        "print('MAPE: ', '{:.5f}%'.format(mape * 100))"
      ],
      "metadata": {
        "id": "Do8rGD28cnFs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}